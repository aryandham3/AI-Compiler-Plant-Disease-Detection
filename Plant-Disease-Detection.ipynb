{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad3dd5b1-4b62-442b-8d5b-574ae950369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6065 images belonging to 5 classes.\n",
      "Found 1514 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 363ms/step - accuracy: 0.4532 - loss: 1.3172 - val_accuracy: 0.8071 - val_loss: 0.4851\n",
      "Epoch 2/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 410ms/step - accuracy: 0.8134 - loss: 0.5468 - val_accuracy: 0.8587 - val_loss: 0.3766\n",
      "Epoch 3/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 362ms/step - accuracy: 0.8597 - loss: 0.3928 - val_accuracy: 0.8606 - val_loss: 0.3656\n",
      "Epoch 4/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 369ms/step - accuracy: 0.8728 - loss: 0.3464 - val_accuracy: 0.9095 - val_loss: 0.2339\n",
      "Epoch 5/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 389ms/step - accuracy: 0.9038 - loss: 0.2636 - val_accuracy: 0.9188 - val_loss: 0.2237\n",
      "Epoch 6/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 489ms/step - accuracy: 0.9216 - loss: 0.2285 - val_accuracy: 0.9240 - val_loss: 0.1908\n",
      "Epoch 7/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 519ms/step - accuracy: 0.9308 - loss: 0.1841 - val_accuracy: 0.8976 - val_loss: 0.2775\n",
      "Epoch 8/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 473ms/step - accuracy: 0.9365 - loss: 0.1703 - val_accuracy: 0.9293 - val_loss: 0.2156\n",
      "Epoch 9/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 443ms/step - accuracy: 0.9425 - loss: 0.1612 - val_accuracy: 0.9333 - val_loss: 0.1799\n",
      "Epoch 10/10\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 460ms/step - accuracy: 0.9496 - loss: 0.1343 - val_accuracy: 0.9465 - val_loss: 0.1641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as 'tomato_disease_model.h5'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Step 1: Extracting Dataset\n",
    "dataset_zip = \"CD.zip\"  \n",
    "dataset_path = \"CD\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    with zipfile.ZipFile(dataset_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "image_size = (128, 128)  # Resize images\n",
    "batch_size = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Step 3: CNN Model Definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_data.num_classes, activation=\"softmax\")  # Dynamic number of classes\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "epochs = 10  # Adjust if needed\n",
    "model.fit(train_data, validation_data=val_data, epochs=epochs)\n",
    "\n",
    "# Step 6: Save the Trained Model\n",
    "model.save(\"tomato_disease_model.h5\")\n",
    "\n",
    "print(\"Training complete! Model saved as 'tomato_disease_model.h5'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91542ac2-3063-4e84-8312-82ee82f1afb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
